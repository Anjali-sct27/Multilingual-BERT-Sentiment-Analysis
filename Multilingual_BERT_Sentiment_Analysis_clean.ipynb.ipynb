{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AEnMVIJLzHoY"
      },
      "outputs": [],
      "source": [
        "!pip install nltk scikit-learn pandas numpy matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "Ek8nEVEezr7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "CEhKmGz20RTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+|#\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "data['clean_tweet'] = data['tweet'].apply(clean_text)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "uDM5Bvun0dLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "sVXrBzSh0qMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+|#\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "data['clean_tweet'] = data['tweet'].apply(clean_text)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "gSdwF1pL1GYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['clean_tweet']\n",
        "y = data['label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "HM5AsULU1MWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "OHUm6uts1Q9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "h2pFa7tf1U_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v-HMMHhK1sEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZmADcEJ116K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    text = clean_text(text)\n",
        "    vector = vectorizer.transform([text])\n",
        "    result = model.predict(vector)\n",
        "    return \"Positive üòä\" if result[0] == 1 else \"Negative üò†\"\n"
      ],
      "metadata": {
        "id": "1WtyuabO1_0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"I really love this project, it's amazing!\")\n"
      ],
      "metadata": {
        "id": "AqoMAUVw2EML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text(\"I really love this project, it's amazing!\")\n"
      ],
      "metadata": {
        "id": "vR0zUI2I2VvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "LHZcXHxs2aaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"I really love this project, it's amazing!\")\n"
      ],
      "metadata": {
        "id": "0wrgaMM52fPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "id": "I9c3_5x627UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "eZiz7Brk2_ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "KU-gbfQ53mkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Positive üòä\" if predicted_class == 1 else \"Negative üò†\"\n"
      ],
      "metadata": {
        "id": "ePx_hgqE3xcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_predict_sentiment(\"I really love this project, it's amazing!\")\n"
      ],
      "metadata": {
        "id": "0tkRze7s31jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_predict_sentiment(\"This is the worst experience ever\")\n",
        "bert_predict_sentiment(\"I hate this product\")\n",
        "bert_predict_sentiment(\"Not bad, could be better\")\n",
        "bert_predict_sentiment(\"Absolutely fantastic work!\")\n"
      ],
      "metadata": {
        "id": "h63o1lhB35pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_predict_sentiment(\"This is the worst experience ever\"))\n",
        "print(bert_predict_sentiment(\"I hate this product\"))\n",
        "print(bert_predict_sentiment(\"Not bad, could be better\"))\n",
        "print(bert_predict_sentiment(\"Absolutely fantastic work!\"))\n"
      ],
      "metadata": {
        "id": "Zt6c3kJJ4LQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect sentencepiece\n"
      ],
      "metadata": {
        "id": "9G0UDzOy4-gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "from transformers import MarianMTModel, MarianTokenizer\n"
      ],
      "metadata": {
        "id": "hvRUw1pp8V_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator_model_name = \"Helsinki-NLP/opus-mt-mul-en\"\n",
        "\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(translator_model_name)\n",
        "translator_model = MarianMTModel.from_pretrained(translator_model_name)\n"
      ],
      "metadata": {
        "id": "Qb4chwcs8YOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(text):\n",
        "    inputs = translator_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    translated = translator_model.generate(**inputs)\n",
        "\n",
        "    english_text = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    return english_text\n"
      ],
      "metadata": {
        "id": "YCjbXBQY8fUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multilingual_sentiment(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = \"unknown\"\n",
        "\n",
        "    # If not English, translate\n",
        "    if lang != \"en\":\n",
        "        translated_text = translate_to_english(text)\n",
        "    else:\n",
        "        translated_text = text\n",
        "\n",
        "    sentiment = bert_predict_sentiment(translated_text)\n",
        "\n",
        "    return {\n",
        "        \"original_text\": text,\n",
        "        \"detected_language\": lang,\n",
        "        \"translated_text\": translated_text,\n",
        "        \"sentiment\": sentiment\n",
        "    }\n"
      ],
      "metadata": {
        "id": "yucaHRil8jM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"I really love this project\")\n"
      ],
      "metadata": {
        "id": "6sX9dfbN8o_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡¥à ‡¥™‡µç‡¥∞‡µã‡¥ú‡¥ï‡µç‡¥ü‡µç ‡¥µ‡¥≥‡¥∞‡µÜ ‡¥®‡¥≤‡µç‡¥≤‡¥§‡¥æ‡¥£‡µç\")\n"
      ],
      "metadata": {
        "id": "EyKpl1Zr8vcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡§Ø‡§π ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§¨‡§π‡•Å‡§§ ‡§ñ‡§∞‡§æ‡§¨ ‡§•‡§æ\")\n"
      ],
      "metadata": {
        "id": "-0bxMD4180D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡Æá‡Æ®‡Øç‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Æø ‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\")\n"
      ],
      "metadata": {
        "id": "_Fy986bC85NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡Æá‡Æ®‡Øç‡Æ§ ‡Æö‡Øá‡Æµ‡Øà ‡ÆÆ‡Æø‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Øã‡Æö‡ÆÆ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\")\n"
      ],
      "metadata": {
        "id": "i7As2Itj9lZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡¥à ‡¥Ö‡¥®‡µÅ‡¥≠‡¥µ‡¥Ç ‡¥§‡µÄ‡¥∞‡µÜ ‡¥Æ‡µã‡¥∂‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥∞‡µÅ‡¥®‡µç‡¥®‡µÅ\")\n"
      ],
      "metadata": {
        "id": "b45iPPG991LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"‡∞à ‡∞Ø‡∞æ‡∞™‡±ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ï‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø\")\n"
      ],
      "metadata": {
        "id": "737HCMgR-DXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"Cette application est vraiment g√©niale\")\n"
      ],
      "metadata": {
        "id": "2zVBDCpI-YQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"Esta aplicaci√≥n es terrible\")\n"
      ],
      "metadata": {
        "id": "P5Yl_cxv-cK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"Diese App ist sehr n√ºtzlich\")\n"
      ],
      "metadata": {
        "id": "UU6oSF0Z-is7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"„Åì„ÅÆ„Ç¢„Éó„É™„ÅØÊúÄÊÇ™„Åß„Åô\")\n"
      ],
      "metadata": {
        "id": "T7bb3-9K-nHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_sentiment(\"Ïù¥ Ïï±ÏùÄ Ï†ïÎßê Ï¢ãÏïÑÏöî\")\n"
      ],
      "metadata": {
        "id": "86PMGSMu-rcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}